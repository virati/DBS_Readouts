{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readout modeling\n",
    "## Full sophistication\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb517d7a7128433ca7983b6f81533c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='coverage', max=1.0, step=0.01), FloatSlider(value=0.â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "import scipy.signal as signal\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(110)\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "matplotlib.rcParams['figure.figsize'] = [20, 15]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "\n",
    "\n",
    "def try_coverage(coverage=1.0,injection=0.0, noise_level=0.0):\n",
    "    behav_dim = 30\n",
    "    brain_dim = 50\n",
    "    measure_dim = 2\n",
    "\n",
    "    gamma_ratio = 1/50\n",
    "    gamma_b = np.random.choice([0,1],size=(behav_dim,brain_dim),p=[1-gamma_ratio,gamma_ratio])\n",
    "    #Here we want to determine h_b parametrized by a coverage parameter [0,1]\n",
    "    disease_regions = np.sum(gamma_b,axis=1) #behav x 1\n",
    "    brain_importance = np.sum(gamma_b,axis=0) # bran x 1\n",
    "    \n",
    "    #find all non-zero brain regions\n",
    "    b_i_idx = np.where(brain_importance > 0)[0]\n",
    "    #now take only coverage * b_i_idx regions randomly and that's your h\n",
    "    measurables = np.random.choice(b_i_idx,replace=False,size=(np.ceil(len(b_i_idx) * coverage).astype(np.int),))\n",
    "    #print(brain_importance)\n",
    "    #print(measurables)\n",
    "\n",
    "    h_b = np.zeros(shape=(len(measurables),brain_dim))\n",
    "    for ii,br in enumerate(measurables):\n",
    "        h_b[ii][br] = 1\n",
    "\n",
    "    injection = 0\n",
    "    #now do injection (including non-disease related brain regions into measurements)\n",
    "    b_n_idx = np.where(brain_importance == 0)[0] #brain, NOT important\n",
    "    #print(b_n_idx)\n",
    "    noise_sources = np.random.choice(b_n_idx,replace=False,size=(np.ceil(len(b_n_idx) * injection).astype(np.int),))\n",
    "    for ii,br in enumerate(noise_sources):\n",
    "        h_b[ii][br] = 1\n",
    "    \n",
    "    #h_b = gamma_b\n",
    "    #h_b = np.random.choice([0,1],size=(measure_dim,brain_dim),p=[2/3,1/3])\n",
    "\n",
    "    N = 200\n",
    "    x = np.random.normal(0,1,size=(brain_dim,N))\n",
    "\n",
    "    beta = np.dot(gamma_b,x)\n",
    "    c = np.sum(beta,axis=0)\n",
    "    y = (np.dot(h_b,x))\n",
    "    y += np.random.normal(0,noise_level,size=y.shape)\n",
    "\n",
    "    readout_model = LinearRegression().fit(y.T,c)\n",
    "\n",
    "    Nt = 1000\n",
    "    x_test = np.random.normal(0,1,size=(brain_dim,Nt))\n",
    "    beta_test = np.dot(gamma_b,x_test)\n",
    "    c_test = np.sum(beta_test,axis=0)\n",
    "    y_test = np.dot(h_b,x_test)\n",
    "\n",
    "    pred_c = readout_model.predict(y_test.T)\n",
    "\n",
    "    plt.subplot(221)\n",
    "    plt.scatter(pred_c,c_test)\n",
    "    plt.xlabel('Predicted');plt.ylabel('Actual')\n",
    "    assess_lr = LinearRegression(fit_intercept=True).fit(pred_c.reshape(-1,1),c_test.reshape(-1,1))\n",
    "    r2_fit_pva = r2_score(c_test,pred_c) #have to be careful here because the function takes (actual, then predicted) in arguments\n",
    "    plt.title('Slope'+str(assess_lr.coef_))\n",
    "    plt.plot([-10,10],[-10,10],linestyle='dotted')\n",
    "\n",
    "    plt.subplot(222)\n",
    "    plt.scatter(c_test,pred_c)\n",
    "    plt.xlabel('Actual');plt.ylabel('Predicted')\n",
    "    assess_rl = LinearRegression(fit_intercept=True).fit(c_test.reshape(-1,1),pred_c.reshape(-1,1))\n",
    "    r2_fit_avp = r2_score(pred_c,c_test)\n",
    "    plt.title('Slope'+str(assess_rl.coef_))\n",
    "    plt.plot([-10,10],[-10,10],linestyle='dotted')\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.imshow(h_b)\n",
    "    \n",
    "    plt.suptitle('R2:' + str((r2_fit_pva,'vs',r2_fit_avp)))\n",
    "    \n",
    "readout_widg = interactive(try_coverage,coverage=(0.0,1.0,0.01),noise_level=(0.0,10.0,0.1))\n",
    "display(readout_widg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example **coverage** refers to what percentage of the number of brain regions known to be involved in behavior are actually being recorded from.\n",
    "\n",
    "**Injection** refers to how many brain regions known to *not* be involved in the disease mapping are being recorded from.\n",
    "\n",
    "Noise is the additive noise component in the final measurement of $\\vec{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working through the math\n",
    "Let's work through the math behind the difference a bit.\n",
    "\n",
    "### Actual-on-Y\n",
    "Putting the 'Actual' Value on the y-axis is convention in machine learning approaches.\n",
    "This is largely because the goal is to assess how something we have access to and can calculate (predicted) maps to the single \n",
    "\n",
    "When we do 'Actual vs Predicted' (In the Y-vs-X convention) we're finding $m$:\n",
    "\n",
    "$$ \\beta = m \\cdot \\hat{\\beta}$$\n",
    "\n",
    "If $\\hat{\\beta}$ has any internal component that is correlated with $\\beta$ then it's trivial to find a scaling within the calculation of $\\hat{\\beta}$ that can admit $m = 1$.\n",
    "So the slope doesn't really mean much.\n",
    "\n",
    "You have no control over A\n",
    "\n",
    "### Actual-on-X\n",
    "\n",
    "$m$ corresponds to the slope . In the probabilistic framework:\n",
    "\n",
    "$$ \\beta \\sim \\mathcal{N}(m\\cdot\\hat{\\beta},\\sigma^2)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
